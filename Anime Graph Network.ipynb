{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "from googleapiclient.discovery import build\n",
    "import datetime as dt\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n",
    "import pickle\n",
    "import difflib\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flow should work like this:\n",
    "\n",
    "1) Anime is google scraped since MAL seems to put some random number in the url\n",
    "\n",
    "2) Get MAL link, scrape everything from this page that is of interest (name, categories, episode count?)\n",
    "\n",
    "3) Add /userrecs to the MAL url and scrape the user recommendations\n",
    "\n",
    "anime --> google_search --> get_name --> get_description --> episode_count --> get_cat --> MAL score --> get_stream --> get_recs \n",
    "\n",
    "How should it be stored?\n",
    "\n",
    "- Dictionary of list of lists: {Anime: [description, episode count, year, [categories], MAL Score,[streaming platforms], [recommended anime]]}\n",
    "\n",
    "Questions\n",
    "\n",
    "- Should I just use the streaming anime list (~1200 anime) to build my network? It'll be extremely large, maybe too large?\n",
    "\n",
    "- Maybe start small and go from there for proof of concept\n",
    "\n",
    "Batch Processing:\n",
    "\n",
    "- First, get the MAL urls from google searches through 100 per batch processing, then urls with names in dictionary, pickle full dictionary to have at hand\n",
    "\n",
    "- Second, go through MAL urls in 100 per batch and scrape MAL site for information and recommendations now that all anime should be key in dictionary (avoid double counting anime from recommendations that are already in anime list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_log(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        time_start = dt.datetime.now()\n",
    "        result = func(*args, **kwargs)\n",
    "        time_end = dt.datetime.now()\n",
    "        print(f\"Function Took: {time_end - time_start}\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Gets google search page, given search term (CHANGED 01/17)\n",
    "# Example of google_search usage\n",
    "#results = google_search(\"Attack on Titan MAL\", my_api_key, my_cse_id)\n",
    "@time_log\n",
    "def google_search(search_term, api_key, cse_id, **kwargs):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()\n",
    "    url = res[\"items\"][0][\"link\"]\n",
    "    #url = url + \"/userrecs\"\n",
    "    return url\n",
    "        \n",
    "\n",
    "# Scrapes the description page from the MAL Details page for the given anime\n",
    "# Example: \n",
    "\"\"\"\n",
    "link = \"https://myanimelist.net/anime/40834/Ousama_Ranking\"\n",
    "resp = requests.get(link)\n",
    "parser = 'html.parser'\n",
    "soup = BeautifulSoup(resp.content, parser)\n",
    "description = soup.select(\".pb16~ p\")\n",
    "print(description[0].text)\n",
    "\"\"\"\n",
    "@time_log\n",
    "def get_description(link):\n",
    "    resp = requests.get(link)\n",
    "    parser = 'html.parser'\n",
    "    soup = BeautifulSoup(resp.content, parser)\n",
    "    description = soup.select(\".pb16~ p\")\n",
    "    text_description = description[0].text\n",
    "    #print(text_description)\n",
    "    return text_description\n",
    "\n",
    "\n",
    "# Scrapes the recommendation page from the MAL User Recommendation page for the given anime (CHANGED 01/17/21)\n",
    "#@time_log\n",
    "def get_name(link):\n",
    "    resp = requests.get(link)\n",
    "    parser = 'html.parser'  # or 'lxml' (preferred) or 'html5lib', if installed\n",
    "    soup = BeautifulSoup(resp.content, parser)\n",
    "    if(soup.select(\".title-inherit\")):\n",
    "        name = soup.select(\".title-inherit\")\n",
    "    else:\n",
    "        name = soup.select(\".h1_bold_none strong\")\n",
    "    \n",
    "    nm = name[0].text\n",
    "    #print(nm)\n",
    "    return nm\n",
    "\n",
    "\n",
    "# Gets the number of episodes for the anime\n",
    "@time_log\n",
    "def get_anime_details(link):\n",
    "    resp = requests.get(link)\n",
    "    parser = 'html.parser'  # or 'lxml' (preferred) or 'html5lib', if installed\n",
    "    soup = BeautifulSoup(resp.content, parser)\n",
    "    # get everything from information section\n",
    "    info = [\".spaceit_pad:nth-child({})\".format(i) for i in range(6, 30)]\n",
    "    cat = [soup.select(d) for d in info]\n",
    "    #ct = cat[0].text\n",
    "    episodes = ''\n",
    "    genres = ''\n",
    "    season = ''\n",
    "    for item in cat:\n",
    "        if(item):\n",
    "            print(item[0].text)\n",
    "            if(\"Episode\" in item[0].text):\n",
    "                sentence = ' '.join(item[0].text.split())\n",
    "                episodes = extract_info(sentence, \"Episodes\")\n",
    "            if(\"Genre\" in item[0].text):\n",
    "                sentence = ' '.join(item[0].text.split())\n",
    "                genres = extract_info(sentence, \"Genre\")\n",
    "            if(\"Premiered\" in item[0].text):\n",
    "                sentence = ' '.join(item[0].text.split())\n",
    "                season = extract_info(sentence, \"Premiered\")\n",
    "            # Store theme for case when genre isn't available\n",
    "            if(\"Theme\" in item[0].text):\n",
    "                sentence = ' '.join(item[0].text.split())\n",
    "                themes = extract_info(sentence, \"Theme\")\n",
    "    # check if anime has genre listed on MAL, if not use themes instead\n",
    "    if(genres == ''):\n",
    "        genres = themes\n",
    "    return [episodes, genres, season]\n",
    "\n",
    "\n",
    "def extract_info(sidebar_str, info_type):\n",
    "    if(info_type == \"Episodes\"):\n",
    "        cleaned_str = re.findall(r\"Episodes: (.*)$\", sidebar_str)[0]\n",
    "        return cleaned_str\n",
    "    if(info_type == \"Genre\"):\n",
    "        if(\"Genres:\" in sidebar_str):\n",
    "            cleaned_str = re.findall(r\"Genres: (.*)$\", sidebar_str)[0]\n",
    "            genre_list = cleaned_str.split(\",\")\n",
    "            updated_list = [re.findall(r\"([A-Z].*)[A-Z]\", genre)[0] for genre in genre_list]\n",
    "            return updated_list\n",
    "        elif(\"Genre:\" in sidebar_str):\n",
    "            cleaned_str = re.findall(r\"Genre: (.*)$\", sidebar_str)[0]\n",
    "            updated_list = [re.findall(r\"([A-Z].*)[A-Z]\", cleaned_str)[0]]\n",
    "            return updated_list\n",
    "        else:\n",
    "            return \"\"\n",
    "    if(info_type == \"Theme\"):\n",
    "        if(\"Themes:\" in sidebar_str):\n",
    "            cleaned_str = re.findall(r\"Themes: (.*)$\", sidebar_str)[0]\n",
    "            genre_list = cleaned_str.split(\",\")\n",
    "            updated_list = [re.findall(r\"([A-Z].*)[A-Z]\", genre)[0] for genre in genre_list]\n",
    "            return updated_list\n",
    "        elif(\"Theme:\" in sidebar_str):\n",
    "            cleaned_str = re.findall(r\"Theme: (.*)$\", sidebar_str)[0]\n",
    "            updated_list = [re.findall(r\"([A-Z].*)[A-Z]\", cleaned_str)[0]]\n",
    "            return updated_list\n",
    "        else:\n",
    "            return \"\"\n",
    "    if(info_type == \"Premiered\"):\n",
    "        cleaned_str = re.findall(r\"Premiered: (.*)$\", sidebar_str)[0]\n",
    "        return cleaned_str\n",
    "    return \"Nothing Found\"\n",
    "    \n",
    "\n",
    "# Gets Anime List for Given Service\n",
    "#@time_log\n",
    "def get_stream(anime_name):\n",
    "    if(difflib.get_close_matches(anime_name, stream.keys(), n = 1, cutoff = 0.8)):\n",
    "        anime_match = difflib.get_close_matches(anime_name, stream.keys(), n = 1, cutoff = 0.8)[0]\n",
    "        stream_services = stream[anime_match]\n",
    "    else:\n",
    "        stream_services = []\n",
    "    #print(stream_services)\n",
    "    return stream_services\n",
    "\n",
    "                \n",
    "# Gets the recommendations from other MAL users, needs to be >threshold to add recommendation\n",
    "# Total_votes is the votes for those that have >= threshold, and then rec_links just takes up to the len of total_votes\n",
    "@time_log\n",
    "def get_rec_links(rec_url, threshold):\n",
    "    resp = requests.get(rec_url)\n",
    "    parser = 'html.parser'  # or 'lxml' (preferred) or 'html5lib', if installed\n",
    "    soup = BeautifulSoup(resp.content, parser)\n",
    "    votes = soup.select(\".js-similar-recommendations-button strong\")\n",
    "    total_votes = [int(i.text) for i in votes if int(i.text)>=threshold]\n",
    "    l = list()\n",
    "    check = 0\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if(str(link['href']).startswith(\"/myrecommendations\")):\n",
    "            check = 1\n",
    "            #print(\"I work\")\n",
    "        if(check == 1):\n",
    "            if(link['href'] == \"https://myanimelist.net/topanime.php\"):\n",
    "                break\n",
    "            z = re.search(\"https://myanimelist\\.net/anime/\\d*/(.*)$\", link['href'])\n",
    "            if(z):\n",
    "                if(link[\"href\"] not in l):\n",
    "                    l.append(link[\"href\"])\n",
    "    rec_links = l[0:len(total_votes)]\n",
    "    rec_names = [[get_name(a), a] for a in rec_links]\n",
    "    return rec_names\n",
    "\n",
    "\n",
    "# Cleans url if it has any mistakes in it\n",
    "#@time_log\n",
    "def clean_url(url):\n",
    "    if \"?\" in url:\n",
    "        cleaned_url = re.findall(r\"(.*)\\?\", url)\n",
    "        return cleaned_url\n",
    "    return url\n",
    "\n",
    "def get_mal_sites(anime_list, ani_recs):\n",
    "    i = 0 \n",
    "    for anime in anime_list:\n",
    "        i += 1\n",
    "        if(i % 50 == 0):\n",
    "            print(\"Going to sleep\")\n",
    "            time.sleep(3600 - time.time() % 3600)\n",
    "        anime_search = anime + \" MyAnimeList\"\n",
    "        url = google_search(anime_search, my_api_key, my_cse_id)\n",
    "        url = clean_url(url)\n",
    "        #print(url)\n",
    "        anime_name = get_name(url)\n",
    "        anime_stream = get_stream(anime)\n",
    "        ani_recs[anime_name] = [url,anime_stream]\n",
    "        print(anime_name, i)\n",
    "    #return ani_recs\n",
    "\n",
    "# Runner function\n",
    "@time_log\n",
    "def anime_runner(anime_list, ani_recs):\n",
    "    full_recs = []\n",
    "    for anime in anime_list:\n",
    "        anime_search = anime + \" MyAnimeList\"\n",
    "        url = google_search(anime_search, my_api_key, my_cse_id)\n",
    "        #print(url)\n",
    "        url = clean_url(url)\n",
    "        #print(url)\n",
    "        anime_name = get_name(url)\n",
    "        anime_description = get_description(url)\n",
    "        anime_episodes, anime_genres, anime_season = get_anime_details(url)\n",
    "        #print(anime_episodes, anime_genres, anime_season)\n",
    "        anime_stream = get_stream(anime)\n",
    "        #Create separate list for the recommended list and then go through same process without get_recs to create profile for \n",
    "        recs_url = url + \"/userrecs\"\n",
    "        anime_recs_url = get_rec_links(recs_url, 10)\n",
    "        anime_recs = [name for name,url in anime_recs_url]\n",
    "        #print(anime_recs)\n",
    "        #break\n",
    "        full_recs.extend(anime_recs_url)\n",
    "        ani_recs[anime_name] = [anime_description, anime_episodes, anime_genres, anime_season, anime_stream, anime_recs]\n",
    "    print(full_recs)\n",
    "    for anime,url in full_recs:\n",
    "        if anime not in ani_recs.keys():\n",
    "            #print(anime)\n",
    "            anime_description = get_description(url)\n",
    "            anime_episodes, anime_genres, anime_season = get_anime_details(url)\n",
    "            anime_stream = get_stream(anime)\n",
    "            recs_url = url + \"/userrecs\"\n",
    "            anime_recs_url = get_rec_links(recs_url, 10)\n",
    "            anime_recs = [name for name,url in anime_recs_url]\n",
    "            ani_recs[anime] = [anime_description, anime_episodes, anime_genres, anime_season, anime_stream, anime_recs]\n",
    "    return ani_recs\n",
    "\n",
    "\n",
    "\n",
    "# Pickle and store object\n",
    "@time_log\n",
    "def pickle_list(df, file_name):\n",
    "    with open(file_name, 'wb') as handle:\n",
    "        pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetworkX Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create anime networkx object \n",
    "@time_log\n",
    "def create_ani_network(ani_recs):\n",
    "    nx_obj = nx.Graph(ani_recs)\n",
    "    #print(\"Fate Zero\" in list(nx_obj.nodes()))\n",
    "    #nx_obj.edges()\n",
    "    #for edge in nx_obj.edges():\n",
    "        #print(edge[0], edge[1])\n",
    "        #print(ani_recs[edge[0]][edge[1]])\n",
    "    #nx_obj.nodes['title'] = list(nx_obj.nodes())\n",
    "    g = Network(height = 800, width = 800, notebook = True)\n",
    "    #g.toggle_hide_edges_on_drag(False)\n",
    "    #g.barnes_hut()\n",
    "    g.from_nx(nx.Graph(ani_recs))\n",
    "    g.set_options(\"\"\"\n",
    "    var options = {\n",
    "      \"nodes\": {\n",
    "        \"font\": {\n",
    "          \"size\": 17,\n",
    "          \"background\": \"rgba(255,255,255,1)\"\n",
    "        }\n",
    "      },\n",
    "      \"edges\": {\n",
    "        \"color\": {\n",
    "          \"inherit\": true\n",
    "        },\n",
    "        \"smooth\": false\n",
    "      },\n",
    "      \"interaction\": {\n",
    "        \"hover\": true,\n",
    "        \"keyboard\": {\n",
    "          \"enabled\": true\n",
    "        },\n",
    "        \"navigationButtons\": true\n",
    "      },\n",
    "      \"physics\": {\n",
    "        \"forceAtlas2Based\": {\n",
    "          \"springLength\": 100\n",
    "        },\n",
    "        \"minVelocity\": 0.75,\n",
    "        \"solver\": \"forceAtlas2Based\"\n",
    "      }\n",
    "    }\n",
    "    \"\"\")\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API and Search Keys\n",
    "my_api_key = \"AIzaSyBRvu5CNwemS1HV3IcQ1bcCnfS30yUaiEM\"\n",
    "my_cse_id = \"7191dc5676ef525ac\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani_recs = dict()\n",
    "\n",
    "stream_list = pd.read_pickle(r'anime_streaming.pickle')\n",
    "stream = {re.sub('\\*|!|:|-|\\+|\\.|\\?|\\^|\\$|\\(|\\)|\\[|\\]|\\{|\\}', '', x): v \n",
    "     for x, v in stream_list.items()}\n",
    "\n",
    "ani_list = stream.keys()\n",
    "\n",
    "#ani_list_pt1 = [\"Attack on Titan\", \"Fullmetal Alchemist Brotherhood\", \"Haikyuu\", \"Demon Slayer\", \"Hunter x Hunter 2011\",\"Noragami\", \"One Piece\", \"Kono Suba\", \"Cowboy Bepop\", \"Vinland Saga\"] \n",
    "#ani_list_pt2 = [\"Code Geass\", \"Great Teacher Onizuka\", \"Mushishi\", \"Boku no Hero Academia\", \"One Punch Man\", \"Tokyo Ghoul\"] \n",
    "#ani_list_pt3 =[\"Bleach\", \"Mob Psycho 100\", \"Psycho-Pass\",\"Fate Zero\", \"JoJo no Kimyou na Bouken TV\", \"Oregairu\"] \n",
    "#ani_list_pt4 = [\"Seishun Buta Yarou\", \"Love is War\", \"Kuroko no Basket\", \"Gintama\", \"Baccano!\", \"Kekkai Sensen\", \"Steins Gate\"]\n",
    "#ani_list_pt5 =[\"March Comes in Like a Lion\", \"Your Lie in April\",\"Monster\", \"Re:Zero\", \"Death Note\"]\n",
    "#ani_list = [\"Great Teacher Onizuka\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sleep\n"
     ]
    },
    {
     "ename": "HttpError",
     "evalue": "<HttpError 429 when requesting https://customsearch.googleapis.com/customsearch/v1?q=Monster+Strike+MyAnimeList&cx=7191dc5676ef525ac&key=AIzaSyBRvu5CNwemS1HV3IcQ1bcCnfS30yUaiEM&alt=json returned \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:436991763619'.\". Details: \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:436991763619'.\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3ba1d592d235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_mal_sites\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mani_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mani_recs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-0030b10e7cb7>\u001b[0m in \u001b[0;36mget_mal_sites\u001b[0;34m(anime_list, ani_recs)\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0manime_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manime\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" MyAnimeList\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogle_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manime_search\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_api_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_cse_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m#print(url)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-0030b10e7cb7>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mtime_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtime_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Function Took: {time_end - time_start}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-0030b10e7cb7>\u001b[0m in \u001b[0;36mgoogle_search\u001b[0;34m(search_term, api_key, cse_id, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgoogle_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcse_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mservice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"customsearch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeveloperKey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_term\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcse_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"items\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"link\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m#url = url + \"/userrecs\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 429 when requesting https://customsearch.googleapis.com/customsearch/v1?q=Monster+Strike+MyAnimeList&cx=7191dc5676ef525ac&key=AIzaSyBRvu5CNwemS1HV3IcQ1bcCnfS30yUaiEM&alt=json returned \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:436991763619'.\". Details: \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:436991763619'.\">"
     ]
    }
   ],
   "source": [
    "get_mal_sites(ani_list, ani_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shonen Maid',\n",
       " 'Big Windup',\n",
       " 'Tales from Earthsea',\n",
       " 'Mobile Suit Gundam',\n",
       " 'Meiji Tokyo Renka',\n",
       " 'Gunslinger Stratos The Animation',\n",
       " 'Soul Eater',\n",
       " 'Batman Ninja',\n",
       " 'Earl and Fairy',\n",
       " 'Good Luck Girl',\n",
       " 'Black Cat',\n",
       " \"Yamada's First Time B Gata H Kei\",\n",
       " 'Fate/Zero',\n",
       " 'The Rolling Girls',\n",
       " 'Ringing Bell',\n",
       " 'Pandora Hearts',\n",
       " \"Sekai Ichi Hatsukoi  World's Greatest First Love\",\n",
       " 'Glasslip',\n",
       " 'Ahiru no Sora',\n",
       " \"Akiba's Trip The Animation\",\n",
       " 'Girlish Number',\n",
       " 'Grimoire of Zero',\n",
       " 'Wise Man’s Grandchild',\n",
       " 'Concrete Revolutio',\n",
       " 'Kemono Friends',\n",
       " 'Rurouni Kenshin',\n",
       " 'A Town Where You Live',\n",
       " 'Beautiful Bones Sakurako’s Investigation',\n",
       " 'Elfen Lied',\n",
       " 'Angolmois Record of Mongol Invasion',\n",
       " 'Puzzle & Dragons X',\n",
       " 'Prince of Stride Alternative',\n",
       " 'The Moment You Fall in Love',\n",
       " 'Tokyo Revengers',\n",
       " 'Anohana The Flower We Saw That Day',\n",
       " 'Chain Chronicle The Light of Haecceitas',\n",
       " 'Project Blue Earth SOS',\n",
       " 'Aoharu x Machinegun',\n",
       " 'Fate/kaleid liner Prisma Illya',\n",
       " 'The Garden of Sinners',\n",
       " 'Hakata Mentai Pirikarakochan',\n",
       " 'Last Exile',\n",
       " 'XMen',\n",
       " \"Howl's Moving Castle\",\n",
       " 'Ninja Girl & Samurai Master',\n",
       " 'Corpse Princess',\n",
       " 'World Break Aria of Curse for a Holy Swordsman',\n",
       " 'SHIROBAKO',\n",
       " \"Masamunekun's Revenge\",\n",
       " 'The Royal Tutor']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_list(ani_recs, \"anime_mal_sites.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://myanimelist.net/anime/14719/JoJo_no_Kimyou_na_Bouken_TV?suggestion=/userrecs\n"
     ]
    }
   ],
   "source": [
    "# Problem Children: My Hero Academia, Jojos\n",
    "# My hero academia search term only pulls up season 3 result first for some reason, so manually input it\n",
    "# For some reason, jojo's adds ?suggestion= into the url, manually input it\n",
    "#url = google_search(\"Jojo's Bizzare Adventure MAL\", my_api_key, my_cse_id)\n",
    "#print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anime Network Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"800\"\n",
       "            src=\"ex.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff874f96850>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#g = create_ani_network(ani_recs)\n",
    "#g.show(\"ex.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Pickling Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle_list(ani_recs, \"anime_recs.pickle\")\n",
    "#pickle_list(watch_on, \"stream_service.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unused Function: 01/16/22\n",
    "\n",
    "@time_log\n",
    "def get_stream_service(url, offset):\n",
    "    resp = requests.get(url)\n",
    "    #search_term = \".css-1u7zfla a\"\n",
    "    search_term = \".e1qyeclq4 p\"\n",
    "    # get BeautifulSoup object\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    # Get the titles of the user recommended animes\n",
    "    links = soup.select(search_term)\n",
    "    recs = [title.text for title in links]\n",
    "    for i in range(50,offset+1,50):\n",
    "        url = url + \"?offset=\" + str(i)\n",
    "        resp = requests.get(url)\n",
    "        # get BeautifulSoup object\n",
    "        soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        # Get the titles of the user recommended animes\n",
    "        links = soup.select(search_term)\n",
    "        recs.extend([title.text for title in links])\n",
    "    return recs\n",
    "\n",
    "# Creates anime recommendation list with given anime list\n",
    "@time_log\n",
    "def add_anime(anime_list, ani_recs):\n",
    "    for anime in anime_list:\n",
    "        anime_search = anime + \" MAL\"\n",
    "        if(anime == \"Boku no Hero Academia\"):\n",
    "            url = \"https://myanimelist.net/anime/31964/Boku_no_Hero_Academia/userrecs\"\n",
    "        elif(anime == \"JoJo no Kimyou na Bouken TV\"):\n",
    "            url = \"https://myanimelist.net/anime/14719/JoJo_no_Kimyou_na_Bouken_TV/userrecs\"\n",
    "        else:\n",
    "            url = google_search(anime_search, my_api_key, my_cse_id)\n",
    "        rec_names = searchRecs(url, anime)\n",
    "        ani_recs[anime] = rec_names\n",
    "        print(anime, \"Done\")\n",
    "\n",
    "\n",
    "# Gets categories (sports, slice of life, etc) of anime\n",
    "@time_log\n",
    "def get_category(link):\n",
    "    resp = requests.get(link)\n",
    "    parser = 'html.parser'  # or 'lxml' (preferred) or 'html5lib', if installed\n",
    "    soup = BeautifulSoup(resp.content, parser)\n",
    "    cat = soup.select(\"span+ a:nth-child(3)\")\n",
    "    ct = cat[0].text\n",
    "    print(ct)\n",
    "    return ct\n",
    "\n",
    "@time_log\n",
    "def searchRecs(res, anime):\n",
    "    if(anime == \"Boku no Hero Academia\"):\n",
    "        url = \"https://myanimelist.net/anime/31964/Boku_no_Hero_Academia/userrecs\"\n",
    "    elif(anime == \"JoJo no Kimyou na Bouken TV\"):\n",
    "        url = \"https://myanimelist.net/anime/14719/JoJo_no_Kimyou_na_Bouken_TV/userrecs\"\n",
    "    else:\n",
    "        url = res[\"items\"][0][\"link\"]\n",
    "        url = url + \"/userrecs\"\n",
    "    #print(\"Step 0\")\n",
    "    rec_links = get_rec_links(url, 10)\n",
    "    #print(\"Step 1\")\n",
    "    #print(rec_links)\n",
    "    anime_name = get_name(url)\n",
    "    \n",
    "    # connect to webpage\n",
    "    resp = requests.get(url)\n",
    "    # get BeautifulSoup object\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    # Get the titles of the user recommended animes\n",
    "    #links = soup.select(\"#content div:nth-child(2) strong\")\n",
    "    #recs = [title.text for title in links]\n",
    "    # Get the upvotes for the given recommendations\n",
    "    #votes = soup.select(\".js-similar-recommendations-button strong\")\n",
    "    # Only keep those that were upvoted at least 5 times\n",
    "    #total_votes = [int(i.text) for i in votes if int(i.text)>=10]\n",
    "    rec_names = [[get_name(a), a] for a in rec_links]\n",
    "    #print(\"Step 2\")\n",
    "    #total_recs = rec_names[0:len(total_votes)]\n",
    "    #ani_recs[anime_name] = rec_names\n",
    "    return(rec_names)\n",
    "    \n",
    "    \n",
    "crunch_list = get_stream_service(\"https://reelgood.com/source/crunchyroll\", 600)\n",
    "netflix_list = get_stream_service(\"https://reelgood.com/genre/anime/on-netflix\", 100)\n",
    "prime_list = get_stream_service(\"https://reelgood.com/genre/anime/on-amazon\", 50)\n",
    "funimation_list = get_stream_service(\"https://reelgood.com/source/funimation\", 550)\n",
    "hbo_list = get_stream_service(\"https://reelgood.com/genre/anime/on-hbo_max\", 50)\n",
    "hulu_list = get_stream_service(\"https://reelgood.com/genre/anime/on-hulu\", 150)\n",
    "\n",
    "url = \"https://reelgood.com/source/crunchyroll\"\n",
    "resp = requests.get(url) \n",
    "offset = 600\n",
    "#search_term = \".css-1u7zfla a\"\n",
    "#search_term = \"p\"\n",
    "search_term = \".e1qyeclq4 p\"\n",
    "# get BeautifulSoup object\n",
    "soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "# Get the titles of the user recommended animes\n",
    "links = soup.select(search_term)\n",
    "#print(links)\n",
    "#print([title.text for title in links])\n",
    "recs = [title.text for title in links]\n",
    "for i in range(50,offset+1,50):\n",
    "    url = url + \"?offset=\" + str(i)\n",
    "    resp = requests.get(url)\n",
    "    # get BeautifulSoup object\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    # Get the titles of the user recommended animes\n",
    "    links = soup.select(search_term)\n",
    "    recs.extend([title.text for title in links])\n",
    "    \n",
    "print(recs)\n",
    "#return recs\n",
    "watch_on = dict()\n",
    "streaming_list = [[crunch_list, \"crunchyroll\"],[netflix_list, \"netflix\"], [prime_list, \"amazon prime\"], [funimation_list, \"funimation\"], [hbo_list, \"HBO\"], [hulu_list, \"hulu\"]]\n",
    "for stream_service in streaming_list:\n",
    "    create_stream_list(g, watch_on, stream_service[0], stream_service[1])\n",
    "    \n",
    "watch_on\n",
    "#reel good doesn't have a lot of anime on it, \n",
    "#so get more comprehensive lists (probably just for netflix, crunchyroll, and funimation)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
